\documentclass[11pt,a4paper]{article}
\usepackage[bottom=2.3cm,top=2.3cm,left=2cm,right=2cm]{geometry}
\usepackage{ngerman}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{ae}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage[pdftex]{graphicx}
\usepackage{float}
\usepackage{kpfonts,baskervald}
\usepackage{listings}
\usepackage{hyperref}
\renewcommand{\labelenumi}{\alph{enumi})}
\renewcommand{\labelenumii}{\roman{enumii})}
\newcommand{\Veranstaltung}[0]{Softwareprojekt Algorithmen der Computervision\\
	                           \large{Waggledance Detection}}
\newcommand{\Autoren}[0]{Roman Schulte-Sasse, Simon Auch, Johannes Sauer}

\lstset{language=C++}

%Titel, Autoren
\title{\Veranstaltung}
\author{\Autoren}
\date{\today}
\begin{document}
\maketitle

% Beispiel für Codelisting, zum Testen aukommentieren:

%\begin{lstlisting}
%#include <stdio.h>
%#define N 10
%/* Block
% * comment */
%
%int main()
%{
%	int i;
%
% 	// Line comment.
% 	puts("Hello world!");
%
% 	for (i = 0; i < N; i++)
% 	{
%		puts("LaTeX is also great for programmers!");
% 	}
%
% 	return 0;
%}
%\end{lstlisting}

\section{Hardwaresetup}% Johannes

Bevor wir überhaupt mit dem algorithmischen Teil des Projekts, der Verbesserung bei der Erkennung der Schwänzeltänze,
anfangen konnten, war es zunächst notwendig die passende Hardware zu beschaffen.
Die verwendete Kamera stammte aus dem PlayStation Eye System und wurde schon vor unserem Projektstart ausgewählt um 
die Tänze zu detektieren, da sie eine außergewöhnliche hohe Aufzeichnungsrate von 100 Hz bei gleichzeitig niedrigem Preis bietet.

Leider ist das Objektiv dieser Kamera nicht auf die Maße der Aufbauten um den Bienenstock ausgerichtet.
Bei dem Setup zur Zeit unseres Projekts war geplant die Kamera ca. 70 bis 90 cm von dem Stock entfernt zu installieren.
Für diese Objektweite wurde ein Objektiv mit anderer Brennweite notwendig.
Da bei der PS Eye Kamera kein Austausch von Objektiven vorgesehen ist, musste der Aufbau über dem Sensor mit dem ursprünglichen
Objektiv zusammen entfernt werden und durch einen Neuen ersetzt werden.

\subsection{Kameramount}% Johannes

Da die PS Eye Kamera allgemein bei Bastlern beliebt ist, waren schon Mehrere vor uns auf die Idee gekommen das Kameragehäuse auszutauschen.
Deswegen sind CAD-Modelle zum 3D-Drucken für solche Gehäuse öffentlich verfügbar.
Bei unserem Projekt verwendeten wir ein \href{http://www.thingiverse.com/thing:83754}{Gehäuse für M12-Objektive}.
Wir konnten das Gehäuse mit dem 3D-Drucker der AG Secure Identity ausdrucken, der Druck selbst war problemlos.
Allerdings stellten wir später bei der Verwendung der modifizierten Kamera fest, dass das Schmelzschichtverfahren bei dem Druck
etwas an Genauigkeit zu wünschen übrig liess.
Das Objektiv hatte deshalb in dem Gewinde etwas zu viel Spiel und musste ab und zu nachjustiert werden, wenn das Licht nicht richtig
auf den Sensor fokussiert wurde.
Alles in allem war die Qualität des Gehäuses für unsere Zwecke jedoch ausreichend.

\subsection{Objektive}% Johannes

Die Wahl unserer Objektive war zum einen durch den schon beschriebenen Abstand zum Bienenstock und den M12-Mount beschränkt.
Da der Stock Ausmaße von ca. 30 cm $\times$ 40 cm hatte, ergab sich eine Brennweite von ca. 6 bis 7 mm.
Außerdem sollte das Objektiv das Bild möglichst wenig verzerren und keinen IR-Filter besitzen.
Da die Kombination dieser Eigenschaften etwas speziell ist hatten wir einige Zeit benötigt um eine gute Auswahl zu finden.
Letztendlich wurden wir von dem Versandhändler \href{http://www.lensation.de/}{Lensation} gut beraten und bekamen sogar mehrere Objektive
zum Testen ausgeliehen.
Eins davon passte gut zu den Anforderungen und wurde deswegen gekauft.

\section{Dotdetector}% Simon
Nachdem wir uns das erste mal in den Code und die Arbeit eingearbeitet und profiled haben wurde uns schnell klar, dass:
\begin{enumerate}
\item  Der Dotdetector einen Fehler hat
\item eine Formel genutzt wird welche sehr unintuitiv ist 
\item 90\% der Ausführungszeit auf den Dotdetector fällt
\item Die restlichen Layers das tun was Sie sollen\\
\end{enumerate}
Aufgrund dieser zwei Punkte haben wir uns entschlossen uns hauptsächlich auf den Dotdetector zu konzentrieren.

\subsection{Fehler der Frequenzanalyse-Berechnung}% Simon
Die Berechnung der Frequenzanalyse geschieht mit folgender Formel:
\begin{center}
\begin{equation} 
score(\bar{B}_j^n, r) = \sum_{1\le m\le b} (\bar{B}_j^n(m) * sin_r^m)^2+(\bar{B}_j^n(m) * cos_r^m)^2
\end{equation}
\begin{equation} 
score(\bar{B}_j^n, r) = \sum_{1\le m\le b} (\bar{B}_j^n(m)) ^2 * (sin_r^m) ^2+(\bar{B}_j^n(m)) ^2 * (cos_r^m) ^2
\end{equation}
\begin{equation} 
score(\bar{B}_j^n, r) = \sum_{1\le m\le b} (\bar{B}_j^n(m)) ^2 * ((sin_r^m) ^2+ (cos_r^m) ^2)
\end{equation}
\begin{equation} 
score(\bar{B}_j^n, r) = \sum_{1\le m\le b} (\bar{B}_j^n(m)) ^2
\end{equation}
\end{center}

Das diese Formel keine Frequenzanalyse darstellt dürfte einleuchtend sein, weswegen wir die Formel folgendermaßen korrigieren:
\begin{center}
\begin{equation} 
score(\bar{B}_j^n, r) = \big(\sum_{1\le m\le b} \bar{B}_j^n(m) * sin_r^m\big)^2 +  \big(\sum_{1\le m\le b} \bar{B}_j^n(m) * cos_r^m\big)^2
\end{equation}
\end{center}

Durch diese einfache Änderung stellten wir bereits eine deutlich reduzierte Erkennungsrate von Pixel fest, welche in dem betrachteten Zeitraum \emph{nur} eine große Amplitude hatten (z.B. dunkle Biene auf hellem Holz).

\subsection{Neues Feature zur Bewertung der Dots}% Simon
Da die Formel zur Bewertung eines Pixel mit all seinen berechneten Frequenzen sehr unintuitiv ist, dachten wir uns vielleicht das man hier eine einfachere und womöglich auch bessere Formel finden können sollte.\\
Die alte Formel:
\begin{center}
\begin{equation} 
potential(D_j)=A^n_j*\sum_{r\in FREQS}w(score(\bar{B}_j^n, r)) * score(\bar{B}_j^n,r)
\end{equation}
\end{center}

Wobei die Funktion $w$ angibt, wieviele Scores kleiner sind, d.h. höhere Scores werden stärker, niedrigere schwächer bewertet. Da in keiner Stelle der Arbeit darauf eingegangen wird, wieso diese Formel an dieser Stelle ihren zweck erfüllt haben wir uns etwas einfacheres ausgedacht.

Als erstes haben wir uns entschieden die Amplitude aus dieser Rechnung zu entfernen, da Sie bereits genutzt wird um zu entscheiden ob der Pixel überhaupt als Dot in betracht gezogen wird. Die zweite Überlegung war, dass wir einfach schauen könnten ob ein Score größer als $x$ ist oder nicht.

\begin{center}
\begin{equation} 
\exists r\in FREQS : x\le score(\bar{B}_j^n, r)
\end{equation}
\end{center}

Da diese Funktion schon sehr vielversprechend arbeitete, jedoch nicht immer alle Waggles erkannt hat, haben wir uns entschieden über jeweils zwei benachbarte Frequenzen die entsprechenden Scores zu summieren und für diese testen ob Sie größer $x$ sind. Diese Änderung hatte den Effekt, das nun auch die Bienen erkannt werden welche genau zwischen zwei Frequenzen tanzen.

\subsection{Probleme des neuen Features und der Frequenanalyse}
Das einzige gravierende Problem welches wir mit dem neuen Feature und der korrigierten Frequenanalyse festgestellt haben ist, dass bei ungünstiger Beleuchtung die Bienen beim sogenannten ventilieren erkannt werden, da der Wechsel von reflektiertem Licht und dunkler Wabe auch in den betrachten Frequenzbereich fällt (bzw. ein vielfachens davon).

Die einfachste möglichkeit dieses Problem zu lösen, welche sehr zuverlässig funktionierte, war die Beleuchtung zu ändern. Als Ideal hat sich diffuse Beleuchtung (z.B. Raumbeleuchtung mit Leuchtröhren) herausgestellt.

Jedoch glauben wir auch dass es ansonsten keine Triviale möglichkeit gibt dieses ventilieren und das tanzen auf dieser Ebene zu unterscheiden, da beides auf die gleichen Frequenzen fällt (bzw. vielfache davon). Eine mögliche Lösung in der Nachbearbeitung währe womöglich die örtlichen Abstände erkannter Waggles zu messen, da sich die Bienen während des ventilierens nicht bewegen und die erkannten Waggles entprechend immer an genau der gleichen Stelle sind. Um an dieser Stelle anzusetzen existiert bereits ein Python Prototyp, siehe Python Prototyp.

\subsection{Parallelisierung der Dotdetection}% Johannes

Während der Laufzeitanalyse des bestehenden Codes ist uns aufgefallen, dass die große Verarbeitungsdauer des Dotdetectorlayers schon bei kleineren Abweichung von den üblichen Parametern (z.B. bei einer höheren Auflösung) zu Problemen führen kann, insbesondere weil dadurch Videoframes verloren werden, was die Erkennung von Tänzen verfälschen kann.\\\
Um diesen Fehler zu beheben versuchten wir zunächst kleinere, lokale Änderungen einzuführen um zu sehen ob sich das Problem ohne aufwendiges Refactoring beheben ließ.\\
Nach mehrern Profilingdurchgängen war uns aufgefallen, dass vor allem die Dotdetection zeitraubend ist, da für jedes Bildfeld, das einen Dot repräsentiert bei der Detektion ein Schleifendurchlauf nötig ist.
Um diesen Teil des Codes zu beschleunigen, parallelisierten wir diese Schleife.
Hierzu verwendeten wir eine {\tt parallel\_for} Schleife; die kritischen Programmteile innerhalb der Schleife wurden durch Locks geschützt.
Diese Verbesserung war zwar sehr schnell durchgeführt und änderte nichts an der Funktionsweise des Programms, leider war der Geschwindigkeitszugewinn jedoch zu klein um das Problem zu lösen.\\

\subsection{Dotdetector mit Matrizen}% Roman

Der Dot-Detector-Layer ist der mit Abstand teuerste Teil der kompletten Waggle-Detection, denn er konsumiert ca. 90 Prozent der gesamten Zeit pro Frame.
Dieser Zeitverbrauch liegt vor Allem an der sehr aufwändigen Projektion jedes einzelnen Pixels auf die Sinus bzw. Cosinus-Funktion.
Der bisherige Ansatz zur Lösung dieses Problems bestand aus einem Objekt pro Pixel, in welches die Helligkeitswerte, die dieser Pixel besaß, kopiert wurden.\\
Für einen neuen Frame wurden also zunächst die Werte aktualisiert und anschließend wurde für jeden einzelnen Pixel die Projektion vorgenommen und entschieden, ob ein Schwellwert überschritten wurde.
Da die Bienen meist in nur 3-4 Frequenzen tanzen, muss dementsprechend jeder Pixel achtmal projeziert (Sinus und Cosinus) und anschließend evaluiert werden.

Um diesen Prozess effizienter zu gestalten, wollten wir diesen Ansatz ändern. Anstatt eines Dot-Detectors pro Pixel sollte die gesamten Bildinformationen über die Zeit in einer Matrix gespeichert werden.
Die Projektion der gesamten Pixel auf einen Sinus oder Cosinus entspricht damit einer Matrix-Multiplikation.
Die Projektion der einzelnen Bildpunkte auf Cosinus/Sinus lässt sich also folgendermaßen formulieren:

\begin{align}
\mathcal{D} \cdot F_{sin} &= p_{sin} \\
\mathcal{D} \cdot F_{cos} &= p_{cos} \\
\end{align}
wobei
\begin{align*}
\mathcal{D} &\in (resX \cdot resY) \times bufferSize \\
F_{sin/cos} &\in bufferSize \times freqNumber
\end{align*}
Dabei steht \textit{resX} die Auflösung des Bildes in horizontaler Richtung, \textit{resY} die Auflösung in vertikaler Richtung und \textit{bufferSize} die Anzahl der im Ringbuffer enthaltenen Bilder. \textit{freqNumber} bezeichnet die Anzahl der Frequenzen, auf die projeziert werden soll.\\
Für alle Frequenzen sind damit nur noch zwei Matrix-Multiplikationen nötig. Dies bietet zwar keine Änderung in der Anzahl der Berechnungen, allerdings sind die modernen Bibliotheken für Lineare Algebra sehr stark optimiert, so dass die Berechnung mittels Matrizen deutlich schneller geht, als eine selbst implementierte Variante.\\
Je nach Prozessor ist der Grad der Optimierung auch unterschiedlich, allerdings sind mittels Armadillo Beschleunigungen im 10-100 fachen erwartbar, da SSE Register und spezielle Makro-Befehle genutzt werden können.\\
Weiterhin bietet diese Form der Berechnung der Projektion eine Möglichkeit auch in der Zukunft weiter zu optimieren, beispielsweise durch Auslagerung der Rechnungen auf eine GPU.\\
In Performance-Tests war die Implementierung mit Matrix-Multiplikation ca. ein Drittel schneller, als die auf den Zweck hochoptimierte alte Variante des Dot-Detectors.\\
Zukünftig könnten auch die Sinus- bzw. Cosinus Matrizen einmal vorberechnet werden, anstatt wie momentan jeden Frame neu. Dadurch ließe sich ein weiterer Speedup der Berechnungen erreichen.

\section{Winkelanalyse}% Roman

Der dritte Layer der \textit{Waggle Detection} beschäftigt sich mit der Extraktion von Winkel und Länge des Tanzes.
In der bisherigen Implementierung wurde dafür der Winkel des letzten und ersten erkannten Punktes, welcher zum Tanz gehört, berechnet. Für die Länge wurde ebenso die euklidische Distanz dieser beiden Punkte zueinander berechnet.
Mathematisch lässt sich die alte Winkelextraktion folgendermaßen aufschreiben:
\begin{align}
\theta &= atan2((last.y-first.y), (last.x-first.x)) \\
r &= \sqrt{(last.x-first.x)^2 + (last.y-first.y)^2}
\end{align}
Dieser Ansatz ist zwar mathematisch korrekt, allerdings gegenüber Ausreißern in der Detektion der Pixel nicht robust.
Weiterhin kann es durchaus sein, dass sich eine Biene immer noch tanzt, während sie bereits dabei ist, sich umzudrehen. In diesem Fall wäre zwar die Richtung des Großteils des Tanzes in einer Richtung, allerdings könnten diese letzten Punkte die extrahierte Richtung sehr stark beeinflussen.

Aus diesem Grund haben wir uns entschieden, auch hier Verbesserungen vorzunehmen, um die endgültige Extraktion der Positionen der Nahrungsquellen für die Bienen zu verbessern.

\subsection{Fitline und Lineare Regression}% Roman

Um die Winkel-Extraktion zu verbessern, haben wir zunächst einen klassischen Algorithmus aus der Mustererkennung verwendet. Die \textit{lineare Regression} berechnet aus verschiedenen Datenpunkten eine Ausgleichsgerade (im $\mathbf{R}^2$). Diese ist zwar nicht sehr robust gegenüber Ausreißern, allerdings schon deutlich besser, als die einfache Extraktion via erstem und letztem Punkt.
Bei der linearen Regression wird der quadratische Fehler einer Gerade durch die Datenpunkte minimiert, was zu einer Gerade führt, die den tatsächlichen Verlauf des Tanzes sehr viel besser approximiert, als der bisherige Algorithmus.

Da allerdings auch dieses Verfahren aus oben genannten Gründen nicht völlig zufriedenstellend war, haben wir mittels der Fitline-Funktion von OpenCV versucht, bessere Ergebnisse zu bekommen.
Diese Funktion basiert auf einer Klasse von Schätzern, sog. \textit{M-Estimators} und verwendet zur Approximation der Linie einen \textit{RANSAC}-ähnlichen Algorithmus.
Durch die Verwendung dieser Funktion ließ sich der Winkel deutlich besser extrahieren als in der Vorgänger-Funktion.

\subsection{Darstellung in Videos}% Roman

Um die Extraktoren gut und angewandt vergleichen zu können, war ein neuer Modus im Programm vonnöten. Durch eine neue Funktion ist es möglich, den Verlauf des Videos für eine bestimmte Zeit automatisch anzuhalten.\\
Während dieses \textit{freezes} des Videos werden die berechneten Winkel der verschiedenen Extraktoren in Form von verschiedenfarbigen Geraden angezeigt.
Dadurch wird eine vergleichsweise einfache Validierung der Winkel-Extraktion ermöglicht. Natürlich ist es möglich, diese Visualisierung auszuschalten, wenn das Programm ativ läuft.

\section{Python Prototyp}% Simon
Gegen Ende des Projektes haben wir noch an einem kleinen Python-Skript gearbeitet, um die gefundenen Waggles zu visualisieren und anhand von Bedingungen (z.B. der maximale/minimale zeitliche und örtliche Abstand von einzelnen Waggles) zu Clustern. Die Visualisierung erfolgt hierbei mithilfe von gnuplot. Zu beachten ist, dass das Skript zur Zeit noch \emph{alle} Waggles die es im angegebenen Ordner findet lädt, dies kann bei vielen Waggles entsprechend lange dauern.

Dieses Projekt ist vermutlich eine gute Stelle um Probleme wie die des neuen Features zur Dotdetection mit ungünstigen Lichverhältnissen in der nachbearbeitung zu lösen.
\subsection{Funktionsübersicht}% Simon
\begin{enumerate}
\item eine Klasse Waggle, welche alle Informationen zu einzelnen Waggles speichert
\item laden aller gefundener Waggles des WDD
\item Clustern der gefunden Waggles anhand von bedingungen (siehe getAllClusters.py, Funktion match)
\item Ausgabe aller gefundenen Waggles/Cluster in eine .csv
\item Visualisierung von Waggles/Clustern\\
\end{enumerate}

\section{Korrektur des Sanitychecks}
Während einigen Tests ist uns aufgefallen, dass man nicht auf der kompletten auflösung von Videos arbeiten konnte da ein Fehlerhafter Sanitycheck auf eine Reduzierung auf mindestens 1/4 bestanden hat.


\end{document}
